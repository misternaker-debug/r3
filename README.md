GRPO R³ — это модификация GRPO для улучшения языковых моделей через обучение с подкреплением, комбинирующая метод GRPO с подходом self-reflection. 
Сначала модели предлагается выполнить задачу. Если она справляется, мы ничего не делаем, так как модель уже соответствует нашим требованиям. Если же она не справляется, мы предлагаем ей проанализировать, что могло пойти не так. более пробно можно почитать в статье: https://arxiv.org/pdf/2505.24726 \
В целях повышении эффективности мы не обучали модель на полном наборе данных, а подготовили датасет ошибок модели. для каждого промта мы попросили модель сгенерировать несколько ответов.\
скрипт для создания датасета: python create_failures.py data.json model_name num_prompts\
скрипт обучения модели: python /home/aminnimullin/projects/R3/custom_grpo.py /home/aminnimullin/failures_dataset.json Qwen/Qwen3-1.7B\
<img width="690" height="314" alt="image" src="https://github.com/user-attachments/assets/22708836-40d9-4b57-8c69-85f5e16cc37f" />
