GRPO R³ — это система для улучшения языковых моделей через обучение с подкреплением, комбинирующая метод Group Relative Policy Optimization (GRPO) с подходом Reasoning via Reinforced Rewards (R³). Проект фокусируется на создании "датасета ошибок" — сгенерированных неправильных ответов, которые используются для обучения модели анализировать и исправлять собственные ошибки.
